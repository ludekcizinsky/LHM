_wandb:
    value:
        cli_version: 0.23.0
        e:
            m96a9anj0ucvqw7r4d1hykhdg36lhcmn:
                args:
                    - --output_dir=/scratch/izar/cizinsky/thesis/preprocessing/taichi/lhm
                    - --scene_name=taichi
                    - --epochs=10
                    - --batch_size=5
                    - --exp_name=dev
                codePath: LHM/finetune_multi_humans.py
                codePathLocal: LHM/finetune_multi_humans.py
                cpu_count: 40
                cpu_count_logical: 40
                cudaVersion: "12.2"
                disk:
                    /:
                        total: "53660876800"
                        used: "8406462464"
                email: ludekcizinsky11@gmail.com
                executable: /scratch/izar/cizinsky/venvs/lhm/bin/python
                git:
                    commit: 285d9a37a4457e7c3d56d1855f18e1b26a2b4839
                    remote: git@github.com:ludekcizinsky/LHM.git
                gpu: Tesla V100-PCIE-32GB
                gpu_count: 1
                gpu_nvidia:
                    - architecture: Volta
                      cudaCores: 5120
                      memoryTotal: "34359738368"
                      name: Tesla V100-PCIE-32GB
                      uuid: GPU-0c983f73-3e90-2236-71bf-769ecd5f5968
                host: i30
                memory:
                    total: "201910206464"
                os: Linux-5.14.0-70.30.1.el9_0.x86_64-x86_64-with-glibc2.34
                program: /home/cizinsky/LHM/LHM/finetune_multi_humans.py
                python: CPython 3.10.19
                root: /home/cizinsky/LHM
                slurm:
                    job_id: "2737298"
                startedAt: "2025-11-29T16:35:50.635617Z"
                writerId: m96a9anj0ucvqw7r4d1hykhdg36lhcmn
        m: []
        python_version: 3.10.19
        t:
            "1":
                - 1
                - 11
                - 41
                - 49
                - 50
                - 63
                - 71
                - 79
                - 83
                - 105
            "2":
                - 1
                - 11
                - 41
                - 49
                - 50
                - 63
                - 71
                - 79
                - 83
                - 105
            "3":
                - 2
                - 16
            "4": 3.10.19
            "5": 0.23.0
            "6": 4.37.2
            "8":
                - 2
            "12": 0.23.0
            "13": linux-x86_64
batch_size:
    value: 5
epochs:
    value: 10
grad_clip:
    value: 0.1
lr:
    value: 0.0004
weight_decay:
    value: 0.0005
